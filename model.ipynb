{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1aWhj_4Ej918gUb93LOPNIcnSNeni5F5j",
      "authorship_tag": "ABX9TyMfdHzCRpt4tnOA8ax02Evm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasirgunes/wind_power_estimation/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries and implement preprocessing**\n",
        "---"
      ],
      "metadata": {
        "id": "JWIW36_uAw8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import seaborn as sns\n",
        "import os # for path manipulation\n",
        "import sys # for path manipulation\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "2pUsrNhoIJiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('drive/MyDrive/Wind Power Estimation')\n",
        "from data_analysis import implement_data_preprocessing"
      ],
      "metadata": {
        "id": "WFbvECVyfBUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aliaga_data, camlica_data, bursa_data, balikesir_data = implement_data_preprocessing()"
      ],
      "metadata": {
        "id": "hAK0Of4kI-qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the wind power data**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oum31BZb9UZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wind_data = aliaga_data[\"wind\"][:\"20180101\"]\n",
        "wind_data.plot()"
      ],
      "metadata": {
        "id": "o5VzqtcX9t2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aliaga_data.head()"
      ],
      "metadata": {
        "id": "KlKkEN4DBN7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup the model**\n",
        "---\n",
        "- It will use sliding window approach\n",
        "- It will for example take 5 hour input one by one and then predict the 6th hour.\n",
        "- The model will be trained this way. It may take 24 hour as input and can predict the 1 hour later. It will be tentative.\n"
      ],
      "metadata": {
        "id": "a38JB0FK-iBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import Sequence"
      ],
      "metadata": {
        "id": "CwyB0wJQHc2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expanding Window Approach**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6NHyyFPwJBwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpandingWindowSequence(Sequence):\n",
        "    \"\"\"\n",
        "    An advanced sequence generator that implements expanding window validation.\n",
        "    This generator starts with a base window of data and gradually expands it\n",
        "    during training, allowing the model to learn from increasingly more data\n",
        "    patterns over time.\n",
        "\n",
        "    The expansion happens by increasing the validation window size after each\n",
        "    epoch, giving the model access to more diverse scenarios as training progresses.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, input_window, prediction_horizon, batch_size=32,\n",
        "                 initial_window_size=None, expansion_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the expanding window sequence generator.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : pd.DataFrame\n",
        "            The complete dataset\n",
        "        input_window : int\n",
        "            Number of time steps to use as input\n",
        "        prediction_horizon : int\n",
        "            How many steps ahead to predict\n",
        "        batch_size : int\n",
        "            Size of each batch\n",
        "        initial_window_size : int\n",
        "            Size of the initial validation window (in data points)\n",
        "        expansion_rate : float\n",
        "            Rate at which to expand the window after each epoch\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.input_window = input_window\n",
        "        self.prediction_horizon = prediction_horizon\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize window parameters\n",
        "        self.initial_window_size = (initial_window_size if initial_window_size\n",
        "                                  else len(data) // 4)  # Start with 25% of data\n",
        "        self.current_window_size = self.initial_window_size\n",
        "        self.expansion_rate = expansion_rate\n",
        "\n",
        "        # Calculate sequences based on current window\n",
        "        self.update_sequences()\n",
        "\n",
        "        # Get feature names (all columns except target)\n",
        "        self.feature_names = [col for col in data.columns if col != 'wind']\n",
        "\n",
        "    def update_sequences(self):\n",
        "        \"\"\"Update the number of sequences based on current window size\"\"\"\n",
        "        self.n_sequences = min(\n",
        "            self.current_window_size - self.input_window - self.prediction_horizon + 1,\n",
        "            len(self.data) - self.input_window - self.prediction_horizon + 1\n",
        "        )\n",
        "\n",
        "    def expand_window(self):\n",
        "        \"\"\"Expand the window size for the next epoch\"\"\"\n",
        "        expansion_size = int(self.initial_window_size * self.expansion_rate)\n",
        "        self.current_window_size = min(\n",
        "            self.current_window_size + expansion_size,\n",
        "            len(self.data)\n",
        "        )\n",
        "        self.update_sequences()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(self.n_sequences / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = min((idx + 1) * self.batch_size, self.n_sequences)\n",
        "\n",
        "        batch_size = batch_end - batch_start\n",
        "        X_batch = np.zeros((batch_size, self.input_window, len(self.feature_names)))\n",
        "        y_batch = np.zeros(batch_size)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            sequence_start = batch_start + i\n",
        "            sequence_end = sequence_start + self.input_window\n",
        "\n",
        "            X_batch[i] = self.data[self.feature_names].iloc[sequence_start:sequence_end].values\n",
        "\n",
        "            target_idx = sequence_end + self.prediction_horizon - 1\n",
        "            y_batch[i] = self.data['wind'].iloc[target_idx]\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "class ExpandingWindowCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Custom callback to handle the expansion of the validation window after each epoch.\n",
        "    This callback works with the ExpandingWindowSequence to gradually increase\n",
        "    the amount of validation data available during training.\n",
        "    \"\"\"\n",
        "    def __init__(self, val_sequence):\n",
        "        super().__init__()\n",
        "        self.val_sequence = val_sequence\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"Expand the validation window after each epoch\"\"\"\n",
        "        self.val_sequence.expand_window()\n",
        "        print(f\"\\nValidation window size expanded to: {self.val_sequence.current_window_size}\")\n",
        "\n",
        "def prepare_data_for_expanding_window(data, input_window, prediction_horizon,\n",
        "                                    initial_window_size=None, batch_size=32):\n",
        "    \"\"\"\n",
        "    Prepares data for expanding window validation as described in the paper.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        Input data containing features and wind power\n",
        "    input_window : int\n",
        "        Number of time steps for input sequence (paper uses 8 hours)\n",
        "    prediction_horizon : int\n",
        "        How many hours ahead to predict (paper uses 1 hour)\n",
        "    initial_window_size : int\n",
        "        Size of initial training window (paper suggests starting with 25% of data)\n",
        "    batch_size : int\n",
        "        Batch size for training\n",
        "    \"\"\"\n",
        "    # Scale the data first\n",
        "    feature_scaler = StandardScaler()\n",
        "    target_scaler = StandardScaler()\n",
        "\n",
        "    feature_names = [col for col in data.columns if col != 'wind']\n",
        "    scaled_features = feature_scaler.fit_transform(data[feature_names])\n",
        "    scaled_target = target_scaler.fit_transform(data[['wind']])\n",
        "\n",
        "    # Create scaled dataframe\n",
        "    scaled_data = pd.DataFrame(scaled_features, columns=feature_names)\n",
        "    scaled_data['wind'] = scaled_target\n",
        "\n",
        "    if initial_window_size is None:\n",
        "        initial_window_size = len(data) // 4  # Start with 25% as suggested\n",
        "\n",
        "    return ExpandingWindowSequence(\n",
        "        scaled_data,\n",
        "        input_window=input_window,\n",
        "        prediction_horizon=prediction_horizon,\n",
        "        batch_size=batch_size,\n",
        "        initial_window_size=initial_window_size\n",
        "    ), target_scaler\n",
        "\n",
        "def train_with_expanding_window(model, data_sequence, epochs=100):\n",
        "    \"\"\"\n",
        "    Trains the model using expanding window validation as per the paper.\n",
        "    \"\"\"\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n",
        "        ExpandingWindowCallback(data_sequence)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        data_sequence,\n",
        "        validation_data=data_sequence,  # Same sequence for validation\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "ypNTSlQeJGgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Z3af97NwLpZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "def build_encoder_decoder_model(input_shape):\n",
        "    \"\"\"\n",
        "    Creates an advanced bidirectional LSTM model for wind power prediction.\n",
        "\n",
        "    This model implements a sophisticated architecture using Bidirectional LSTM layers,\n",
        "    which process sequences both forward and backward. This bidirectional approach\n",
        "    allows the model to capture temporal patterns that might be missed when only\n",
        "    processing in one direction, which is particularly valuable for wind power\n",
        "    prediction where both leading and trailing weather patterns matter.\n",
        "\n",
        "    The architecture includes:\n",
        "    1. Two Bidirectional LSTM layers for deep feature extraction\n",
        "    2. Batch normalization for training stability\n",
        "    3. Dropout layers for preventing overfitting\n",
        "    4. Dense layers for final prediction refinement\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        Shape of input data in format (time_steps, features)\n",
        "        Example: (48, 10) for 48 hours of historical data with 10 features\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : tf.keras.Model\n",
        "        Compiled bidirectional LSTM model ready for training\n",
        "\n",
        "    Architecture Details:\n",
        "    -------------------\n",
        "    - First Bidirectional LSTM: 128 units (64 in each direction)\n",
        "        Processes the input sequence in both directions to capture temporal dependencies\n",
        "\n",
        "    - Second Bidirectional LSTM: 64 units (32 in each direction)\n",
        "        Refines the features extracted by the first layer\n",
        "\n",
        "    - Batch Normalization: After each LSTM layer\n",
        "        Normalizes layer outputs to stabilize training and reduce internal covariate shift\n",
        "\n",
        "    - Dropout (0.2): After each batch normalization\n",
        "        Randomly deactivates 20% of units during training to prevent overfitting\n",
        "\n",
        "    - Dense Layers: Final prediction refinement\n",
        "        Converts processed features into wind power predictions\n",
        "\n",
        "    Notes:\n",
        "    ------\n",
        "    - The model uses 'tanh' activation in LSTM layers (default) which is suitable\n",
        "      for capturing both positive and negative patterns in wind power changes\n",
        "    - ReLU activation in dense layers helps capture non-linear relationships\n",
        "    - Learning rate is set to 0.001 for stable training\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Bidirectional(LSTM(128, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),  # Added another layer\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(32)),  # Final layer\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Configure optimizer with reduced learning rate for stability\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    # Compile model with mean squared error loss and mean absolute error metric\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mse',  # Mean Squared Error for regression\n",
        "        metrics=['mae']  # Mean Absolute Error for interpretability\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_WUTWCAeLrv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model_performance(model, val_gen, target_scaler, num_samples=None):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance using multiple metrics and visualization.\n",
        "\n",
        "    This function:\n",
        "    1. Collects predictions and actual values from the validation generator\n",
        "    2. Calculates R², RMSE, MSE, and MAE\n",
        "    3. Creates visualization of predicted vs actual values\n",
        "    4. Prints a detailed performance report\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tf.keras.Model\n",
        "        The trained model to evaluate\n",
        "    val_gen : WindPowerSequence\n",
        "        Validation data generator\n",
        "    num_samples : int, optional\n",
        "        Number of samples to evaluate (defaults to entire validation set)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing all computed metrics\n",
        "    \"\"\"\n",
        "    # Collect predictions and actual values\n",
        "    true_values = []\n",
        "    predictions = []\n",
        "\n",
        "    # If num_samples not specified, use all available samples\n",
        "    if num_samples is None:\n",
        "        num_samples = len(val_gen) * val_gen.batch_size\n",
        "\n",
        "    samples_collected = 0\n",
        "\n",
        "    # Collect predictions batch by batch\n",
        "    for i in range(len(val_gen)):\n",
        "        if samples_collected >= num_samples:\n",
        "            break\n",
        "\n",
        "        X_batch, y_batch = val_gen[i]\n",
        "        batch_predictions = model.predict(X_batch, verbose=0)\n",
        "\n",
        "        true_values.extend(y_batch)\n",
        "        predictions.extend(batch_predictions.flatten())\n",
        "\n",
        "        samples_collected += len(y_batch)\n",
        "\n",
        "    # Convert lists to numpy arrays first\n",
        "    true_values = np.array(true_values)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # Now we can reshape and inverse transform\n",
        "    true_values = target_scaler.inverse_transform(true_values.reshape(-1, 1)).flatten()\n",
        "    predictions = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    r2 = r2_score(true_values, predictions)\n",
        "    mse = mean_squared_error(true_values, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(true_values, predictions)\n",
        "\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Time series plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(true_values, label='Actual', alpha=0.7)\n",
        "    plt.plot(predictions, label='Predicted', alpha=0.7)\n",
        "    plt.title('Actual vs Predicted Wind Power')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Wind Power')\n",
        "    plt.legend()\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(true_values, predictions, alpha=0.5)\n",
        "    plt.plot([true_values.min(), true_values.max()],\n",
        "             [true_values.min(), true_values.max()],\n",
        "             'r--', label='Perfect Prediction')\n",
        "    plt.title('Predicted vs Actual Values')\n",
        "    plt.xlabel('Actual Wind Power')\n",
        "    plt.ylabel('Predicted Wind Power')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed report\n",
        "    print(\"\\nModel Performance Metrics:\")\n",
        "    print(\"-------------------------\")\n",
        "    print(f\"R² Score: {r2:.4f} {'✓' if r2 > 0.7 else '✗'} (Target: > 0.7)\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "    if r2 > 0.7:\n",
        "        print(\"\\n✓ Model meets the R² requirement (> 0.7)\")\n",
        "    else:\n",
        "        print(\"\\n✗ Model does not meet the R² requirement (> 0.7)\")\n",
        "        print(\"Consider applying some improvements:\")\n",
        "        print(\"1. Increase the input window size\")\n",
        "        print(\"2. Add batch normalization\")\n",
        "        print(\"3. Adjust the model architecture\")\n",
        "        print(\"4. Feature selection/engineering\")\n",
        "\n",
        "    return {\n",
        "        'r2': r2,\n",
        "        'rmse': rmse,\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'predictions': predictions,\n",
        "        'true_values': true_values\n",
        "    }"
      ],
      "metadata": {
        "id": "XhQrjL8a_Ecf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def complete_wind_power_evaluation():\n",
        "    \"\"\"\n",
        "    Implements the complete three-stage evaluation process from the paper:\n",
        "    1. Train and validate on one wind farm\n",
        "    2. Test on other well-established wind farms\n",
        "    3. Test on newly-established wind farm\n",
        "    \"\"\"\n",
        "    # Split aliaga_data into training and validation sets\n",
        "    train_split = 0.8\n",
        "    train_data = aliaga_data.iloc[:int(len(aliaga_data) * train_split)]\n",
        "    val_data = aliaga_data.iloc[int(len(aliaga_data) * train_split):]\n",
        "\n",
        "    # Prepare separate data sequences for training and validation\n",
        "    train_sequence, target_scaler = prepare_data_for_expanding_window(\n",
        "        train_data, input_window=8, prediction_horizon=1, batch_size=32\n",
        "    )\n",
        "    val_sequence, _ = prepare_data_for_expanding_window(\n",
        "        val_data, input_window=8, prediction_horizon=1, batch_size=32\n",
        "    )\n",
        "\n",
        "    # # Stage 1: Train on Aliaga wind farm\n",
        "    # print(\"Stage 1: Training on Aliaga Wind Farm\")\n",
        "    # data_sequence, target_scaler = prepare_data_for_expanding_window(\n",
        "    #     aliaga_data,\n",
        "    #     input_window=8,\n",
        "    #     prediction_horizon=1,\n",
        "    #     batch_size=32\n",
        "    # )\n",
        "\n",
        "    # Build and train model\n",
        "    input_shape = (8, len([col for col in aliaga_data.columns if col != 'wind']))\n",
        "    model = build_encoder_decoder_model(input_shape)\n",
        "    history = train_with_expanding_window(model, data_sequence)\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Stage 2: Test on other well-established wind farms\n",
        "    print(\"\\nStage 2: Testing on Other Well-established Wind Farms\")\n",
        "\n",
        "    print(\"\\nTesting on Balikesir Wind Farm:\")\n",
        "    # Scale the Balikesir data using the same scalers as the training data\n",
        "    feature_names = [col for col in balikesir_data.columns if col != 'wind']\n",
        "\n",
        "    feature_scaler = StandardScaler()\n",
        "    feature_scaler.fit(aliaga_data[feature_names])  # Fit the feature scaler on Aliaga data\n",
        "\n",
        "    scaled_features = feature_scaler.transform(balikesir_data[feature_names])\n",
        "    scaled_target = target_scaler.transform(balikesir_data[['wind']])\n",
        "\n",
        "    # Prepare scaled Balikesir data\n",
        "    scaled_balikesir_data = pd.DataFrame(scaled_features, columns=feature_names)\n",
        "    scaled_balikesir_data['wind'] = scaled_target\n",
        "\n",
        "    # Create the test sequence for Balikesir\n",
        "    test_sequence = ExpandingWindowSequence(\n",
        "        scaled_balikesir_data,\n",
        "        input_window=8,\n",
        "        prediction_horizon=1,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on Balikesir wind farm\n",
        "    true_values = []\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(test_sequence)):\n",
        "        X_batch, y_batch = test_sequence[i]\n",
        "        batch_predictions = model.predict(X_batch, verbose=0)\n",
        "\n",
        "        true_values.extend(y_batch)\n",
        "        predictions.extend(batch_predictions.flatten())\n",
        "\n",
        "    # Convert to arrays and inverse transform\n",
        "    true_values = np.array(true_values)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    true_values = target_scaler.inverse_transform(true_values.reshape(-1, 1)).flatten()\n",
        "    predictions = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    r2 = r2_score(true_values, predictions)\n",
        "    mse = mean_squared_error(true_values, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(true_values, predictions)\n",
        "\n",
        "    # Visualize results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Time series plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(true_values[:1000], label='Actual', alpha=0.7)\n",
        "    plt.plot(predictions[:1000], label='Predicted', alpha=0.7)\n",
        "    plt.title('Actual vs Predicted Wind Power (First 1000 samples)')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Wind Power')\n",
        "    plt.legend()\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(true_values, predictions, alpha=0.1)\n",
        "    plt.plot([true_values.min(), true_values.max()],\n",
        "             [true_values.min(), true_values.max()],\n",
        "             'r--', label='Perfect Prediction')\n",
        "    plt.title('Predicted vs Actual Values')\n",
        "    plt.xlabel('Actual Wind Power')\n",
        "    plt.ylabel('Predicted Wind Power')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nModel Performance on Balikesir Wind Farm:\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'balikesir_metrics': {\n",
        "            'r2': r2,\n",
        "            'rmse': rmse,\n",
        "            'mse': mse,\n",
        "            'mae': mae\n",
        "        },\n",
        "    }"
      ],
      "metadata": {
        "id": "9pbppI6-yTig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the evaluation\n",
        "complete_wind_power_evaluation()"
      ],
      "metadata": {
        "id": "URybpgM9Pttz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}